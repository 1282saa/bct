{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b181e112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ê°€ì„¤ 1 ì™„ì „íŒ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ (í˜„ì„œë‹˜ ë°©ì‹)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "ì£¼ìš” êµ¬í˜„ ì‚¬í•­:\n",
    "1. 5ë‹¨ê³„ ìƒíƒœ ë¼ë²¨ë§ (ì •ìƒì˜ì—…, D-12m, D-9m, D-6m, D-3m)\n",
    "2. êµ¬ê°„í˜• ë³€ìˆ˜ ìë™ ì¸ì½”ë”©\n",
    "3. ê²°ì¸¡ 50% ê¸°ì¤€ ë³€ìˆ˜ ì„ íƒ\n",
    "4. ìµœê·¼ì›” ìŠ¤ëƒ…ìƒ· í´ëŸ¬ìŠ¤í„°ë§\n",
    "5. ì„ë°•ë¹„ì¤‘ ê³„ì‚° (ìœ„í—˜ë„ í‰ê°€)\n",
    "6. LISA ê³µê°„ë¶„ì„ (High-High í•«ìŠ¤íŒŸ)\n",
    "7. í´ëŸ¬ìŠ¤í„°ë³„ ìœ„í—˜ë„ ì¢…í•© í‰ê°€\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ê°€ì„¤ 1 ì™„ì „íŒ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ (í˜„ì„œë‹˜ ë°©ì‹)\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "528f7655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "1. ë°ì´í„° ë¡œë“œ ë° ë‚ ì§œ ì²˜ë¦¬\n",
      "================================================================================\n",
      "ë°ì´í„° í˜•íƒœ: (86263, 192)\n",
      "ê¸°ì¤€ë…„ì›” ë²”ìœ„: 2023-01-01 00:00:00 ~ 2024-12-01 00:00:00\n",
      "íì—… ê°€ë§¹ì : 2,334ê°œ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 1. ë°ì´í„° ë¡œë“œ ë° ë‚ ì§œ ì²˜ë¦¬\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. ë°ì´í„° ë¡œë“œ ë° ë‚ ì§œ ì²˜ë¦¬\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "data_path = \"/Users/yeong-gwang/Documents/ë°°ì›€ ì˜¤ì „ 1.38.42/ì™¸ë¶€/ê³µëª¨ì „/ë¹…ì½˜í…ŒìŠ¤íŠ¸/Project/work/ver3_/1009/ë¹…ì½˜í…ŒìŠ¤íŠ¸_ì „ì²´ë³‘í•©ë°ì´í„°_20251008.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# ë‚ ì§œ ë³€í™˜\n",
    "df['ê¸°ì¤€ë…„ì›”_dt'] = pd.to_datetime(df['ê¸°ì¤€ë…„ì›”'].astype(str), format='%Y%m', errors='coerce')\n",
    "\n",
    "# íì—…ì¼ â†’ Period M (í˜„ì„œë‹˜ ë°©ì‹)\n",
    "def _to_periodM_from_any(s):\n",
    "    s = s.astype(str).str.replace(r'\\.0$', '', regex=True).str.strip()\n",
    "    dt = pd.to_datetime(s, errors='coerce')\n",
    "    return dt.dt.to_period('M')\n",
    "\n",
    "df['_ê´€ì¸¡ì›”'] = df['ê¸°ì¤€ë…„ì›”_dt'].dt.to_period('M')\n",
    "df['_íì—…ì›”'] = _to_periodM_from_any(df['íì—…ì¼'])\n",
    "\n",
    "# ê°œì›”ì°¨ ê³„ì‚° (ì–‘ìˆ˜ = íì—… ì „, ìŒìˆ˜ = íì—… í›„)\n",
    "y1, m1 = df['_íì—…ì›”'].dt.year, df['_íì—…ì›”'].dt.month\n",
    "y0, m0 = df['_ê´€ì¸¡ì›”'].dt.year, df['_ê´€ì¸¡ì›”'].dt.month\n",
    "df['_months_to_close'] = (y1 - y0) * 12 + (m1 - m0)\n",
    "\n",
    "print(f\"ë°ì´í„° í˜•íƒœ: {df.shape}\")\n",
    "print(f\"ê¸°ì¤€ë…„ì›” ë²”ìœ„: {df['ê¸°ì¤€ë…„ì›”_dt'].min()} ~ {df['ê¸°ì¤€ë…„ì›”_dt'].max()}\")\n",
    "print(f\"íì—… ê°€ë§¹ì : {df['_íì—…ì›”'].notna().sum():,}ê°œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3380a4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "2. 5ë‹¨ê³„ ìƒíƒœ ë¼ë²¨ë§ (ì •ìƒ, D-12/9/6/3)\n",
      "================================================================================\n",
      "\n",
      "ì •ìƒì˜ì—…: 4,043ê°œ\n",
      "D-12m: 105ê°œ\n",
      "D-9m: 113ê°œ\n",
      "D-6m: 32ê°œ\n",
      "D-3m: 32ê°œ\n",
      "\n",
      "í†µí•© ë°ì´í„°: 4,325ê°œ\n",
      "\n",
      "[ìƒíƒœ ë¶„í¬]:\n",
      "ìƒíƒœ\n",
      "D-12m     105\n",
      "D-3m       32\n",
      "D-6m       32\n",
      "D-9m      113\n",
      "ì •ìƒì˜ì—…     4043\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 2. 5ë‹¨ê³„ ìƒíƒœ ë¼ë²¨ë§ \n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. 5ë‹¨ê³„ ìƒíƒœ ë¼ë²¨ë§ (ì •ìƒ, D-12/9/6/3)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ì •ìƒì˜ì—…: í•œë²ˆë„ íì—… ê¸°ë¡ ì—†ëŠ” ê°€ë§¹ì ì˜ ìµœì‹  ë°ì´í„°\n",
    "closed_any = df.groupby('ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸')['_íì—…ì›”'].transform(lambda s: s.notna().any())\n",
    "normal_latest = (df[~closed_any]\n",
    "                 .sort_values(['ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸', 'ê¸°ì¤€ë…„ì›”_dt'])\n",
    "                 .groupby('ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸', as_index=False)\n",
    "                 .tail(1)\n",
    "                 .assign(ìƒíƒœ='ì •ìƒì˜ì—…'))\n",
    "\n",
    "print(f\"\\nì •ìƒì˜ì—…: {len(normal_latest):,}ê°œ\")\n",
    "\n",
    "# íì—… D-k: ê° ê°€ë§¹ì ì—ì„œ ì •í™•íˆ D-k ê°œì›” ì „ ìŠ¤ëƒ…ìƒ· 1ê±´\n",
    "def _pick_preclose(k):\n",
    "    sub = df[df['_months_to_close'].eq(k)].copy()\n",
    "    if sub.empty:\n",
    "        return sub.assign(ìƒíƒœ=f'D-{k}m')\n",
    "    sub = (sub.sort_values(['ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸', 'ê¸°ì¤€ë…„ì›”_dt'])\n",
    "              .groupby('ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸', as_index=False)\n",
    "              .tail(1))\n",
    "    sub['ìƒíƒœ'] = f'D-{k}m'\n",
    "    return sub\n",
    "\n",
    "# ì„ë°• ì‹œì  ì¶”ì¶œ\n",
    "d12 = _pick_preclose(12)\n",
    "d9 = _pick_preclose(9)\n",
    "d6 = _pick_preclose(6)\n",
    "d3 = _pick_preclose(3)\n",
    "\n",
    "print(f\"D-12m: {len(d12):,}ê°œ\")\n",
    "print(f\"D-9m: {len(d9):,}ê°œ\")\n",
    "print(f\"D-6m: {len(d6):,}ê°œ\")\n",
    "print(f\"D-3m: {len(d3):,}ê°œ\")\n",
    "\n",
    "# í†µí•©\n",
    "ana = pd.concat([normal_latest, d12, d9, d6, d3], ignore_index=True)\n",
    "\n",
    "# ìƒíƒœ ìˆœì„œ ì •ì˜\n",
    "STATUS_ORDER = [\"ì •ìƒì˜ì—…\", \"D-12m\", \"D-9m\", \"D-6m\", \"D-3m\"]\n",
    "STATUS_RANK = {s: i for i, s in enumerate(STATUS_ORDER)}\n",
    "ana['_ìƒíƒœ_ord'] = ana['ìƒíƒœ'].map(STATUS_RANK)\n",
    "\n",
    "print(f\"\\ní†µí•© ë°ì´í„°: {len(ana):,}ê°œ\")\n",
    "print(\"\\n[ìƒíƒœ ë¶„í¬]:\")\n",
    "print(ana['ìƒíƒœ'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42e0a36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "3. êµ¬ê°„í˜• ë³€ìˆ˜ ìë™ ì¸ì½”ë”©\n",
      "================================================================================\n",
      "\n",
      "í›„ë³´ ë³€ìˆ˜ ìˆ˜: 177ê°œ\n",
      "âœ“ ìˆ˜ì¹˜í˜• ë³€í™˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 3. êµ¬ê°„í˜• ë³€ìˆ˜ ìë™ ì¸ì½”ë”©\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. êµ¬ê°„í˜• ë³€ìˆ˜ ìë™ ì¸ì½”ë”©\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def encode_bucket_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"êµ¬ê°„í˜• ë³€ìˆ˜ë¥¼ ìˆ«ìë¡œ ë³€í™˜ (ì •ê·œì‹ ì¶”ì¶œ â†’ ì‹¤íŒ¨ì‹œ ë¹ˆë„ìˆœ)\"\"\"\n",
    "    ser = s.astype(str)\n",
    "    head_num = ser.str.extract(r'^(\\d+)', expand=False)\n",
    "    out = pd.to_numeric(head_num, errors='coerce')\n",
    "\n",
    "    if out.notna().any() and out.isna().mean() < 0.5:\n",
    "        return out\n",
    "\n",
    "    # ì‹¤íŒ¨ ì‹œ ë¹ˆë„ìˆœ\n",
    "    ser = s.fillna('N/A').astype(str)\n",
    "    order = ser.value_counts().index.tolist()\n",
    "    mapping = {v: i+1 for i, v in enumerate(order)}\n",
    "    return ser.map(mapping).astype(float)\n",
    "\n",
    "def to_numeric_smart(df_in: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
    "    \"\"\"ëª¨ë“  ì»¬ëŸ¼ì„ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "    df2 = df_in.copy()\n",
    "    for c in cols:\n",
    "        if c.endswith('êµ¬ê°„'):\n",
    "            df2[c] = encode_bucket_series(df2[c])\n",
    "        else:\n",
    "            df2[c] = pd.to_numeric(df2[c], errors='coerce')\n",
    "        # ì„¼í‹°ë„ ì œê±°\n",
    "        df2.loc[df2[c] < -1e8, c] = np.nan\n",
    "    return df2\n",
    "\n",
    "# ë¶„ì„ í›„ë³´ ë³€ìˆ˜\n",
    "exclude_cols = {\n",
    "    'ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸', 'ê¸°ì¤€ë…„ì›”', 'ê¸°ì¤€ì—°ì›”', 'ê¸°ì¤€ë¶„ê¸°', 'ê°œì„¤ì¼', 'íì—…ì¼',\n",
    "    'ìƒê¶Œ_ì½”ë“œ', 'ìƒê¶Œ_ì½”ë“œ_ëª…', 'êµ¬ë¶„ì§€ì—­', 'ì§€ì—­ëª…',\n",
    "    'ì—…ì¢…', 'ìƒíƒœ', '_ìƒíƒœ_ord', 'ê¸°ì¤€ë…„ì›”_dt',\n",
    "    '_íì—…ì›”', '_ê´€ì¸¡ì›”', '_months_to_close'\n",
    "}\n",
    "\n",
    "cand_cols = [c for c in ana.columns if c not in exclude_cols]\n",
    "print(f\"\\ní›„ë³´ ë³€ìˆ˜ ìˆ˜: {len(cand_cols)}ê°œ\")\n",
    "\n",
    "# ìˆ˜ì¹˜í˜• ë³€í™˜\n",
    "ana_num = to_numeric_smart(ana, cand_cols)\n",
    "\n",
    "# ê¸°ë³¸ ì»¬ëŸ¼ ë³µì›\n",
    "for col in ['ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸', 'ìƒíƒœ', '_ìƒíƒœ_ord', 'ê¸°ì¤€ë…„ì›”_dt']:\n",
    "    if col in ana.columns:\n",
    "        ana_num[col] = ana[col]\n",
    "\n",
    "print(\"âœ“ ìˆ˜ì¹˜í˜• ë³€í™˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "980c98a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "4. ë³€ìˆ˜ ì„ íƒ (ê²°ì¸¡ 50% ê¸°ì¤€)\n",
      "================================================================================\n",
      "\n",
      "ì„ íƒëœ ë³€ìˆ˜ ìˆ˜: 175ê°œ\n",
      "ì¢Œí‘œ ë°ì´í„° ì¡´ì¬: True\n",
      "\n",
      "ë³€ìˆ˜ ìœ í˜•:\n",
      "  - êµ¬ê°„í˜•: 6ê°œ\n",
      "  - ìˆ˜ì¹˜í˜•: 169ê°œ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 4. ë³€ìˆ˜ ì„ íƒ (ê²°ì¸¡ 50% ê¸°ì¤€)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. ë³€ìˆ˜ ì„ íƒ (ê²°ì¸¡ 50% ê¸°ì¤€)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ì¢Œí‘œ ì»¬ëŸ¼ í™•ì¸\n",
    "XCOL, YCOL = 'ì¢Œí‘œì •ë³´(X)', 'ì¢Œí‘œì •ë³´(Y)'\n",
    "has_coords = (XCOL in ana_num.columns) and (YCOL in ana_num.columns)\n",
    "\n",
    "# ê²°ì¸¡ 50% ì´í•˜ì¸ ìˆ˜ì¹˜í˜• ë³€ìˆ˜\n",
    "feat_cols = [c for c in cand_cols\n",
    "             if pd.api.types.is_numeric_dtype(ana_num[c])\n",
    "             and ana_num[c].notna().mean() > 0.5]\n",
    "\n",
    "print(f\"\\nì„ íƒëœ ë³€ìˆ˜ ìˆ˜: {len(feat_cols)}ê°œ\")\n",
    "print(f\"ì¢Œí‘œ ë°ì´í„° ì¡´ì¬: {has_coords}\")\n",
    "\n",
    "# êµ¬ê°„í˜•/ìˆ˜ì¹˜í˜• ë¶„ë¥˜\n",
    "bucket_cols = [c for c in feat_cols if c.endswith('êµ¬ê°„')]\n",
    "numeric_cols = [c for c in feat_cols if not c.endswith('êµ¬ê°„')]\n",
    "\n",
    "print(f\"\\në³€ìˆ˜ ìœ í˜•:\")\n",
    "print(f\"  - êµ¬ê°„í˜•: {len(bucket_cols)}ê°œ\")\n",
    "print(f\"  - ìˆ˜ì¹˜í˜•: {len(numeric_cols)}ê°œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5e3a51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "5. í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„° ì¤€ë¹„\n",
      "================================================================================\n",
      "\n",
      "ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì¤‘...\n",
      "\n",
      "ë³€ìˆ˜ í–‰ë ¬ í˜•íƒœ: (4325, 175)\n",
      "âœ“ ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 5. í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„° ì¤€ë¹„\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„° ì¤€ë¹„\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„°ë§ìš© ë°ì´í„°\n",
    "cluster_df = ana_num[['ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸', 'ìƒíƒœ', '_ìƒíƒœ_ord'] + feat_cols].copy()\n",
    "\n",
    "# ì¢Œí‘œ ì¶”ê°€ (LISAìš©)\n",
    "if has_coords:\n",
    "    cluster_df[XCOL] = ana_num[XCOL]\n",
    "    cluster_df[YCOL] = ana_num[YCOL]\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ì¤‘ì•™ê°’)\n",
    "print(\"\\nê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì¤‘...\")\n",
    "for c in feat_cols:\n",
    "    if cluster_df[c].isna().any():\n",
    "        median_val = cluster_df[c].median(skipna=True)\n",
    "        cluster_df[c] = cluster_df[c].fillna(median_val)\n",
    "\n",
    "# ë³€ìˆ˜ í–‰ë ¬\n",
    "X = cluster_df[feat_cols].values\n",
    "print(f\"\\në³€ìˆ˜ í–‰ë ¬ í˜•íƒœ: {X.shape}\")\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ë§\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"âœ“ ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e43b1250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "6. K-means í´ëŸ¬ìŠ¤í„°ë§ (n_init=30)\n",
      "================================================================================\n",
      "\n",
      "k=3~7 í‰ê°€ ì¤‘...\n",
      "  k=3: Silhouette=0.5956, DB=1.1781\n",
      "  k=4: Silhouette=0.3169, DB=1.2489\n",
      "  k=5: Silhouette=0.2193, DB=1.4818\n",
      "  k=6: Silhouette=0.1614, DB=1.5612\n",
      "  k=7: Silhouette=0.1673, DB=1.4742\n",
      "\n",
      "ìµœì  k: 3 (Silhouette=0.5956)\n",
      "âœ“ í´ëŸ¬ìŠ¤í„° í• ë‹¹ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 6. K-means í´ëŸ¬ìŠ¤í„°ë§\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6. K-means í´ëŸ¬ìŠ¤í„°ë§ (n_init=30)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# k ì„ íƒ\n",
    "k_range = range(3, 8)\n",
    "evaluation_results = []\n",
    "\n",
    "print(\"\\nk=3~7 í‰ê°€ ì¤‘...\")\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, n_init=30, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "    silhouette = silhouette_score(X_scaled, labels)\n",
    "    db_score = davies_bouldin_score(X_scaled, labels)\n",
    "    inertia = kmeans.inertia_\n",
    "\n",
    "    evaluation_results.append({\n",
    "        'k': k,\n",
    "        'silhouette': silhouette,\n",
    "        'davies_bouldin': db_score,\n",
    "        'inertia': inertia\n",
    "    })\n",
    "\n",
    "    print(f\"  k={k}: Silhouette={silhouette:.4f}, DB={db_score:.4f}\")\n",
    "\n",
    "eval_df = pd.DataFrame(evaluation_results)\n",
    "best_k = eval_df.loc[eval_df['silhouette'].idxmax(), 'k']\n",
    "best_silhouette = eval_df.loc[eval_df['silhouette'].idxmax(), 'silhouette']\n",
    "\n",
    "print(f\"\\nìµœì  k: {best_k} (Silhouette={best_silhouette:.4f})\")\n",
    "\n",
    "# ìµœì¢… í´ëŸ¬ìŠ¤í„°ë§\n",
    "kmeans_final = KMeans(n_clusters=best_k, n_init=30, random_state=42)\n",
    "cluster_df['cluster'] = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "print(f\"âœ“ í´ëŸ¬ìŠ¤í„° í• ë‹¹ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8822ec11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "7. ì„ë°•ë¹„ì¤‘ ê³„ì‚° (ìœ„í—˜ë„ í‰ê°€)\n",
      "================================================================================\n",
      "\n",
      "í´ëŸ¬ìŠ¤í„°ë³„ ì„ë°•ë¹„ì¤‘:\n",
      "         ê°€ë§¹ì ìˆ˜    ì •ìƒ  D12  D9  D6  D3      ì„ë°•ë¹„ì¤‘    ìœ„í—˜ë„\n",
      "cluster                                              \n",
      "0         198     0   94  67  20  17  1.000000  ğŸ”´ ê³ ìœ„í—˜\n",
      "2          46     0   11  12  10  13  1.000000  ğŸ”´ ê³ ìœ„í—˜\n",
      "1        4081  4043    0  34   2   2  0.009311   âœ… ì •ìƒ\n",
      "\n",
      "ê³ ìœ„í—˜ í´ëŸ¬ìŠ¤í„°: [0, 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 7. ì„ë°•ë¹„ì¤‘ ê³„ì‚° \n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"7. ì„ë°•ë¹„ì¤‘ ê³„ì‚° (ìœ„í—˜ë„ í‰ê°€)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„°ë³„ ìƒíƒœ ë¶„í¬\n",
    "cluster_risk = cluster_df.groupby('cluster').agg(\n",
    "    ê°€ë§¹ì ìˆ˜=('ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸', 'count'),\n",
    "    ì •ìƒ=('ìƒíƒœ', lambda s: (s == 'ì •ìƒì˜ì—…').sum()),\n",
    "    D12=('ìƒíƒœ', lambda s: (s == 'D-12m').sum()),\n",
    "    D9=('ìƒíƒœ', lambda s: (s == 'D-9m').sum()),\n",
    "    D6=('ìƒíƒœ', lambda s: (s == 'D-6m').sum()),\n",
    "    D3=('ìƒíƒœ', lambda s: (s == 'D-3m').sum())\n",
    ")\n",
    "\n",
    "# ë¹„ìœ¨ ê³„ì‚°\n",
    "cluster_risk['ì •ìƒ_ë¹„ìœ¨'] = cluster_risk['ì •ìƒ'] / cluster_risk['ê°€ë§¹ì ìˆ˜']\n",
    "cluster_risk['D12_ë¹„ìœ¨'] = cluster_risk['D12'] / cluster_risk['ê°€ë§¹ì ìˆ˜']\n",
    "cluster_risk['D9_ë¹„ìœ¨'] = cluster_risk['D9'] / cluster_risk['ê°€ë§¹ì ìˆ˜']\n",
    "cluster_risk['D6_ë¹„ìœ¨'] = cluster_risk['D6'] / cluster_risk['ê°€ë§¹ì ìˆ˜']\n",
    "cluster_risk['D3_ë¹„ìœ¨'] = cluster_risk['D3'] / cluster_risk['ê°€ë§¹ì ìˆ˜']\n",
    "\n",
    "# ì„ë°•ë¹„ì¤‘ (D-12/9/6/3 í•©ê³„)\n",
    "cluster_risk['ì„ë°•ë¹„ì¤‘'] = (\n",
    "    cluster_risk['D12_ë¹„ìœ¨'] +\n",
    "    cluster_risk['D9_ë¹„ìœ¨'] +\n",
    "    cluster_risk['D6_ë¹„ìœ¨'] +\n",
    "    cluster_risk['D3_ë¹„ìœ¨']\n",
    ")\n",
    "\n",
    "# ìœ„í—˜ë„ ë“±ê¸‰ ë¶€ì—¬\n",
    "cluster_risk['ìœ„í—˜ë„'] = cluster_risk['ì„ë°•ë¹„ì¤‘'].apply(\n",
    "    lambda x: 'ğŸ”´ ê³ ìœ„í—˜' if x > 0.05 else ('ğŸŸ¡ ì¤‘ìœ„í—˜' if x > 0.02 else 'âœ… ì •ìƒ')\n",
    ")\n",
    "\n",
    "cluster_risk = cluster_risk.sort_values('ì„ë°•ë¹„ì¤‘', ascending=False)\n",
    "\n",
    "print(\"\\ní´ëŸ¬ìŠ¤í„°ë³„ ì„ë°•ë¹„ì¤‘:\")\n",
    "print(cluster_risk[['ê°€ë§¹ì ìˆ˜', 'ì •ìƒ', 'D12', 'D9', 'D6', 'D3', 'ì„ë°•ë¹„ì¤‘', 'ìœ„í—˜ë„']])\n",
    "\n",
    "# ê³ ìœ„í—˜ í´ëŸ¬ìŠ¤í„° ì‹ë³„\n",
    "high_risk_clusters = cluster_risk[cluster_risk['ì„ë°•ë¹„ì¤‘'] > 0.05].index.tolist()\n",
    "print(f\"\\nê³ ìœ„í—˜ í´ëŸ¬ìŠ¤í„°: {high_risk_clusters if high_risk_clusters else 'ì—†ìŒ'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ee49253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "8. LISA ê³µê°„ë¶„ì„ (í•«ìŠ¤íŒŸ ì‹ë³„)\n",
      "================================================================================\n",
      "\n",
      "ì¢Œí‘œ ë°ì´í„°: 4,325ê°œ\n",
      "âœ“ LISA ë¶„ì„ ì™„ë£Œ\n",
      "\n",
      "í´ëŸ¬ìŠ¤í„°ë³„ High-High ë¹„ìœ¨:\n",
      "            HH_ë¹„ìœ¨  HH_ìœ ì˜\n",
      "cluster                 \n",
      "2        0.929078    131\n",
      "0        0.804734    246\n",
      "1        0.003152    540\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 8. LISA ê³µê°„ë¶„ì„ (High-High í•«ìŠ¤íŒŸ)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"8. LISA ê³µê°„ë¶„ì„ (í•«ìŠ¤íŒŸ ì‹ë³„)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "lisa_result = None\n",
    "\n",
    "if has_coords:\n",
    "    try:\n",
    "        from libpysal.weights import KNN\n",
    "        from esda.moran import Moran_Local\n",
    "        from shapely.geometry import Point\n",
    "        import geopandas as gpd\n",
    "\n",
    "        # ì¢Œí‘œ ìœ íš¨í•œ ë°ì´í„°ë§Œ\n",
    "        df_spatial = cluster_df[\n",
    "            cluster_df[XCOL].notna() &\n",
    "            cluster_df[YCOL].notna()\n",
    "        ].copy()\n",
    "\n",
    "        print(f\"\\nì¢Œí‘œ ë°ì´í„°: {len(df_spatial):,}ê°œ\")\n",
    "\n",
    "        if len(df_spatial) >= 30:  # ìµœì†Œ í‘œë³¸ ìˆ˜\n",
    "            # GeoDataFrame ìƒì„±\n",
    "            df_spatial['geometry'] = df_spatial.apply(\n",
    "                lambda row: Point(row[XCOL], row[YCOL]), axis=1\n",
    "            )\n",
    "            gdf = gpd.GeoDataFrame(df_spatial, geometry='geometry')\n",
    "\n",
    "            # ìœ„í—˜ ì ìˆ˜ ê³„ì‚° (ì„ë°•ë¹„ì¤‘ ê¸°ë°˜)\n",
    "            risk_scores = cluster_df.groupby('cluster')['_ìƒíƒœ_ord'].mean()\n",
    "            df_spatial['risk_score'] = df_spatial['cluster'].map(risk_scores)\n",
    "\n",
    "            # k-NN ê°€ì¤‘ì¹˜ (k=8)\n",
    "            w = KNN.from_dataframe(gdf, k=min(8, len(gdf)-1))\n",
    "\n",
    "            # Local Moran's I\n",
    "            lisa = Moran_Local(df_spatial['risk_score'].values, w)\n",
    "\n",
    "            # ì‚¬ë¶„ë©´ ë¶„ë¥˜\n",
    "            y_z = (df_spatial['risk_score'] - df_spatial['risk_score'].mean()) / df_spatial['risk_score'].std()\n",
    "            lz = lisa.Is - lisa.Is.mean()\n",
    "\n",
    "            quad = np.where((y_z > 0) & (lz > 0), 'High-High',\n",
    "                    np.where((y_z < 0) & (lz < 0), 'Low-Low',\n",
    "                    np.where((y_z > 0) & (lz < 0), 'High-Low',\n",
    "                    np.where((y_z < 0) & (lz > 0), 'Low-High', 'Undefined'))))\n",
    "\n",
    "            df_spatial['LISA_quad'] = quad\n",
    "            df_spatial['LISA_sig'] = lisa.p_sim < 0.05\n",
    "\n",
    "            # ê²°ê³¼ ë³‘í•©\n",
    "            cluster_df = cluster_df.merge(\n",
    "                df_spatial[['ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸', 'LISA_quad', 'LISA_sig']],\n",
    "                on='ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸',\n",
    "                how='left'\n",
    "            )\n",
    "\n",
    "            # High-High ë¹„ìœ¨\n",
    "            lisa_summary = cluster_df.groupby('cluster').agg(\n",
    "                HH_ë¹„ìœ¨=('LISA_quad', lambda s: (s == 'High-High').mean()),\n",
    "                HH_ìœ ì˜=('LISA_sig', lambda s: s.sum())\n",
    "            )\n",
    "\n",
    "            cluster_risk = cluster_risk.merge(lisa_summary, left_index=True, right_index=True)\n",
    "\n",
    "            print(\"âœ“ LISA ë¶„ì„ ì™„ë£Œ\")\n",
    "            print(\"\\ní´ëŸ¬ìŠ¤í„°ë³„ High-High ë¹„ìœ¨:\")\n",
    "            print(lisa_summary.sort_values('HH_ë¹„ìœ¨', ascending=False))\n",
    "\n",
    "            lisa_result = {\n",
    "                'total_points': len(df_spatial),\n",
    "                'high_high_count': (df_spatial['LISA_quad'] == 'High-High').sum(),\n",
    "                'high_high_sig': ((df_spatial['LISA_quad'] == 'High-High') & df_spatial['LISA_sig']).sum()\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            print(f\"âš ï¸ ì¢Œí‘œ ë°ì´í„° ë¶€ì¡± (ìµœì†Œ 30ê°œ í•„ìš”, í˜„ì¬ {len(df_spatial)}ê°œ)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ LISA ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "        print(\"   (libpysal, esda íŒ¨í‚¤ì§€ ì„¤ì¹˜ í•„ìš”: pip install libpysal esda)\")\n",
    "else:\n",
    "    print(\"âš ï¸ ì¢Œí‘œ ë°ì´í„° ì—†ìŒ (LISA ë¶„ì„ ê±´ë„ˆëœ€)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86ae7c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "9. í´ëŸ¬ìŠ¤í„° í”„ë¡œíŒŒì¼ë§\n",
      "================================================================================\n",
      "\n",
      "ë¶„ì‚° ê¸°ì¤€ ìƒìœ„ 10ê°œ ë³€ìˆ˜:\n",
      "  1. ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡\n",
      "  2. ì£¼ì¤‘_ë§¤ì¶œ_ê¸ˆì•¡\n",
      "  3. ì§€ì¶œ_ì´ê¸ˆì•¡\n",
      "  4. ì‹œê°„ëŒ€_17~21_ë§¤ì¶œ_ê¸ˆì•¡\n",
      "  5. ì‹œê°„ëŒ€_11~14_ë§¤ì¶œ_ê¸ˆì•¡\n",
      "  6. ì£¼ë§_ë§¤ì¶œ_ê¸ˆì•¡\n",
      "  7. ì‹œê°„ëŒ€_21~24_ë§¤ì¶œ_ê¸ˆì•¡\n",
      "  8. ê¸ˆìš”ì¼_ë§¤ì¶œ_ê¸ˆì•¡\n",
      "  9. ì‹ë£Œí’ˆ_ì§€ì¶œ_ì´ê¸ˆì•¡\n",
      "  10. ëª©ìš”ì¼_ë§¤ì¶œ_ê¸ˆì•¡\n",
      "\n",
      "í´ëŸ¬ìŠ¤í„°ë³„ í‰ê· :\n",
      "             ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡      ì£¼ì¤‘_ë§¤ì¶œ_ê¸ˆì•¡        ì§€ì¶œ_ì´ê¸ˆì•¡  ì‹œê°„ëŒ€_17~21_ë§¤ì¶œ_ê¸ˆì•¡  \\\n",
      "cluster                                                              \n",
      "0        5.421365e+08  3.977223e+08  9.507291e+08     2.250469e+08   \n",
      "1        8.096208e+08  6.059382e+08  9.613653e+08     3.190997e+08   \n",
      "2        1.471308e+09  1.085152e+09  9.067863e+08     5.808036e+08   \n",
      "\n",
      "         ì‹œê°„ëŒ€_11~14_ë§¤ì¶œ_ê¸ˆì•¡      ì£¼ë§_ë§¤ì¶œ_ê¸ˆì•¡  ì‹œê°„ëŒ€_21~24_ë§¤ì¶œ_ê¸ˆì•¡     ê¸ˆìš”ì¼_ë§¤ì¶œ_ê¸ˆì•¡  \\\n",
      "cluster                                                                 \n",
      "0           1.233092e+08  1.444142e+08     9.087301e+07  9.376870e+07   \n",
      "1           2.111237e+08  2.036832e+08     1.284699e+08  1.374322e+08   \n",
      "2           3.891280e+08  3.861569e+08     2.360576e+08  2.494707e+08   \n",
      "\n",
      "           ì‹ë£Œí’ˆ_ì§€ì¶œ_ì´ê¸ˆì•¡     ëª©ìš”ì¼_ë§¤ì¶œ_ê¸ˆì•¡  \n",
      "cluster                              \n",
      "0        2.590808e+08  8.399966e+07  \n",
      "1        2.634176e+08  1.256317e+08  \n",
      "2        2.480732e+08  2.321746e+08  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 9. í´ëŸ¬ìŠ¤í„° í”„ë¡œíŒŒì¼ë§\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"9. í´ëŸ¬ìŠ¤í„° í”„ë¡œíŒŒì¼ë§\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ìƒìœ„ 10ê°œ ì¤‘ìš” ë³€ìˆ˜\n",
    "feature_variance = cluster_df[feat_cols].var().sort_values(ascending=False)\n",
    "top_features = feature_variance.head(10).index.tolist()\n",
    "\n",
    "print(f\"\\në¶„ì‚° ê¸°ì¤€ ìƒìœ„ 10ê°œ ë³€ìˆ˜:\")\n",
    "for i, feat in enumerate(top_features, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„°ë³„ í‰ê·  (ì›ê°’)\n",
    "cluster_profile = cluster_df.groupby('cluster')[top_features].mean().round(2)\n",
    "\n",
    "print(\"\\ní´ëŸ¬ìŠ¤í„°ë³„ í‰ê· :\")\n",
    "print(cluster_profile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3797495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "10. ê²°ê³¼ ì €ì¥\n",
      "================================================================================\n",
      "âœ“ í´ëŸ¬ìŠ¤í„° í• ë‹¹ ê²°ê³¼ ì €ì¥\n",
      "âœ“ ì„ë°•ë¹„ì¤‘ í†µê³„ ì €ì¥\n",
      "âœ“ k í‰ê°€ ê²°ê³¼ ì €ì¥\n",
      "âœ“ í´ëŸ¬ìŠ¤í„° í”„ë¡œíŒŒì¼ ì €ì¥\n",
      "âœ“ ì¢…í•© ìš”ì•½ JSON ì €ì¥\n",
      "âœ“ ìƒì„¸ ë³´ê³ ì„œ ì €ì¥\n",
      "\n",
      "================================================================================\n",
      "ê°€ì„¤ 1 ì™„ì „íŒ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì™„ë£Œ!\n",
      "================================================================================\n",
      "âœ… 5ë‹¨ê³„ ìƒíƒœ ë¼ë²¨ë§ ì™„ë£Œ\n",
      "âœ… ìµœì  k: 3\n",
      "âœ… Silhouette Score: 0.5956\n",
      "âœ… ê³ ìœ„í—˜ í´ëŸ¬ìŠ¤í„°: 2ê°œ\n",
      "âœ… LISA High-High í•«ìŠ¤íŒŸ: 199ê°œ\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 10. ê²°ê³¼ ì €ì¥\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"10. ê²°ê³¼ ì €ì¥\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "output_dir = \"/Users/yeong-gwang/Documents/ë°°ì›€ ì˜¤ì „ 1.38.42/ì™¸ë¶€/ê³µëª¨ì „/ë¹…ì½˜í…ŒìŠ¤íŠ¸/Project/work/ver3_/1012/result/3_ê°€ì„¤1ë¶„ì„\"\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1) í´ëŸ¬ìŠ¤í„° í• ë‹¹ ê²°ê³¼\n",
    "result_df = cluster_df[['ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸', 'cluster', 'ìƒíƒœ', '_ìƒíƒœ_ord']].copy()\n",
    "if 'LISA_quad' in cluster_df.columns:\n",
    "    result_df['LISA_quad'] = cluster_df['LISA_quad']\n",
    "    result_df['LISA_sig'] = cluster_df['LISA_sig']\n",
    "\n",
    "result_df.to_csv(f\"{output_dir}/í´ëŸ¬ìŠ¤í„°ë§_ê²°ê³¼_ì™„ì „íŒ.csv\", index=False, encoding='utf-8-sig')\n",
    "print(\"âœ“ í´ëŸ¬ìŠ¤í„° í• ë‹¹ ê²°ê³¼ ì €ì¥\")\n",
    "\n",
    "# 2) ì„ë°•ë¹„ì¤‘ í†µê³„\n",
    "cluster_risk.to_csv(f\"{output_dir}/í´ëŸ¬ìŠ¤í„°_ì„ë°•ë¹„ì¤‘_ì™„ì „íŒ.csv\", encoding='utf-8-sig')\n",
    "print(\"âœ“ ì„ë°•ë¹„ì¤‘ í†µê³„ ì €ì¥\")\n",
    "\n",
    "# 3) k í‰ê°€ ê²°ê³¼\n",
    "eval_df.to_csv(f\"{output_dir}/í´ëŸ¬ìŠ¤í„°_ê°œìˆ˜_í‰ê°€_ì™„ì „íŒ.csv\", index=False, encoding='utf-8-sig')\n",
    "print(\"âœ“ k í‰ê°€ ê²°ê³¼ ì €ì¥\")\n",
    "\n",
    "# 4) í´ëŸ¬ìŠ¤í„° í”„ë¡œíŒŒì¼\n",
    "cluster_profile.to_csv(f\"{output_dir}/í´ëŸ¬ìŠ¤í„°_í”„ë¡œíŒŒì¼_ì™„ì „íŒ.csv\", encoding='utf-8-sig')\n",
    "print(\"âœ“ í´ëŸ¬ìŠ¤í„° í”„ë¡œíŒŒì¼ ì €ì¥\")\n",
    "\n",
    "# 5) ì¢…í•© ìš”ì•½ JSON\n",
    "summary = {\n",
    "    \"ë¶„ì„ì¼\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"ë¶„ì„ë°©ë²•\": \"í˜„ì„œë‹˜ ë°©ì‹ ì™„ì „íŒ\",\n",
    "    \"ìƒíƒœë¼ë²¨\": STATUS_ORDER,\n",
    "    \"ìƒíƒœë¶„í¬\": {\n",
    "        \"ì •ìƒì˜ì—…\": int((cluster_df['ìƒíƒœ'] == 'ì •ìƒì˜ì—…').sum()),\n",
    "        \"D-12m\": int((cluster_df['ìƒíƒœ'] == 'D-12m').sum()),\n",
    "        \"D-9m\": int((cluster_df['ìƒíƒœ'] == 'D-9m').sum()),\n",
    "        \"D-6m\": int((cluster_df['ìƒíƒœ'] == 'D-6m').sum()),\n",
    "        \"D-3m\": int((cluster_df['ìƒíƒœ'] == 'D-3m').sum())\n",
    "    },\n",
    "    \"ë³€ìˆ˜ìˆ˜\": len(feat_cols),\n",
    "    \"ìµœì k\": int(best_k),\n",
    "    \"Silhouette_Score\": float(best_silhouette),\n",
    "    \"ê³ ìœ„í—˜_í´ëŸ¬ìŠ¤í„°\": [int(c) for c in high_risk_clusters],\n",
    "    \"í´ëŸ¬ìŠ¤í„°ë³„_ì„ë°•ë¹„ì¤‘\": {\n",
    "        str(int(k)): float(v)\n",
    "        for k, v in cluster_risk['ì„ë°•ë¹„ì¤‘'].items()\n",
    "    }\n",
    "}\n",
    "\n",
    "if lisa_result:\n",
    "    summary['LISAë¶„ì„'] = {\n",
    "        'total_points': int(lisa_result['total_points']),\n",
    "        'high_high_count': int(lisa_result['high_high_count']),\n",
    "        'high_high_sig': int(lisa_result['high_high_sig'])\n",
    "    }\n",
    "\n",
    "with open(f\"{output_dir}/í´ëŸ¬ìŠ¤í„°ë§_ìš”ì•½_ì™„ì „íŒ.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "print(\"âœ“ ì¢…í•© ìš”ì•½ JSON ì €ì¥\")\n",
    "\n",
    "# 6) ìƒì„¸ ë³´ê³ ì„œ\n",
    "report = f\"\"\"# ê°€ì„¤ 1 í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì™„ì „íŒ (í˜„ì„œë‹˜ ë°©ì‹)\n",
    "\n",
    "**ë¶„ì„ì¼**: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "**ë¶„ì„ ëŒ€ìƒ**: ê°€ë§¹ì  {len(cluster_df):,}ê°œ\n",
    "---\n",
    "\n",
    "## ğŸ“Š 1. ë¶„ì„ ê°œìš”\n",
    "\n",
    "### 1.1 5ë‹¨ê³„ ìƒíƒœ ë¼ë²¨ë§\n",
    "\n",
    "| ìƒíƒœ | ê°€ë§¹ì  ìˆ˜ | ë¹„ìœ¨ | ì„¤ëª… |\n",
    "|------|-----------|------|------|\n",
    "| ì •ìƒì˜ì—… | {(cluster_df['ìƒíƒœ']=='ì •ìƒì˜ì—…').sum():,}ê°œ | {(cluster_df['ìƒíƒœ']=='ì •ìƒì˜ì—…').sum()/len(cluster_df)*100:.1f}% | ìµœê·¼ì›” ì •ìƒ ê°€ë§¹ì  |\n",
    "| D-12m | {(cluster_df['ìƒíƒœ']=='D-12m').sum():,}ê°œ | {(cluster_df['ìƒíƒœ']=='D-12m').sum()/len(cluster_df)*100:.1f}% | íì—… 12ê°œì›” ì „ |\n",
    "| D-9m | {(cluster_df['ìƒíƒœ']=='D-9m').sum():,}ê°œ | {(cluster_df['ìƒíƒœ']=='D-9m').sum()/len(cluster_df)*100:.1f}% | íì—… 9ê°œì›” ì „ |\n",
    "| D-6m | {(cluster_df['ìƒíƒœ']=='D-6m').sum():,}ê°œ | {(cluster_df['ìƒíƒœ']=='D-6m').sum()/len(cluster_df)*100:.1f}% | íì—… 6ê°œì›” ì „ |\n",
    "| D-3m | {(cluster_df['ìƒíƒœ']=='D-3m').sum():,}ê°œ | {(cluster_df['ìƒíƒœ']=='D-3m').sum()/len(cluster_df)*100:.1f}% | íì—… 3ê°œì›” ì „ |\n",
    "\n",
    "### 1.2 í´ëŸ¬ìŠ¤í„°ë§ ì„¤ì •\n",
    "\n",
    "- **ì•Œê³ ë¦¬ì¦˜**: K-means\n",
    "- **ë³€ìˆ˜ ìˆ˜**: {len(feat_cols)}ê°œ (ê²°ì¸¡ 50% ê¸°ì¤€)\n",
    "- **ìŠ¤ì¼€ì¼ë§**: StandardScaler\n",
    "- **n_init**: 30 (ì´ˆê¸°ê°’ ì˜ì¡´ì„± ê°ì†Œ)\n",
    "- **ìµœì  k**: {best_k}\n",
    "- **Silhouette Score**: {best_silhouette:.4f}\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ 2. í´ëŸ¬ìŠ¤í„°ë³„ ì„ë°•ë¹„ì¤‘ (ìœ„í—˜ë„)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for cluster_id in cluster_risk.index:\n",
    "    row = cluster_risk.loc[cluster_id]\n",
    "    report += f\"\"\"\n",
    "### í´ëŸ¬ìŠ¤í„° {cluster_id} {row['ìœ„í—˜ë„']}\n",
    "\n",
    "- **ê°€ë§¹ì  ìˆ˜**: {int(row['ê°€ë§¹ì ìˆ˜']):,}ê°œ ({row['ê°€ë§¹ì ìˆ˜']/len(cluster_df)*100:.1f}%)\n",
    "- **ì •ìƒ**: {int(row['ì •ìƒ'])}ê°œ ({row['ì •ìƒ_ë¹„ìœ¨']*100:.1f}%)\n",
    "- **D-12m**: {int(row['D12'])}ê°œ ({row['D12_ë¹„ìœ¨']*100:.1f}%)\n",
    "- **D-9m**: {int(row['D9'])}ê°œ ({row['D9_ë¹„ìœ¨']*100:.1f}%)\n",
    "- **D-6m**: {int(row['D6'])}ê°œ ({row['D6_ë¹„ìœ¨']*100:.1f}%)\n",
    "- **D-3m**: {int(row['D3'])}ê°œ ({row['D3_ë¹„ìœ¨']*100:.1f}%)\n",
    "- **ì„ë°•ë¹„ì¤‘**: {row['ì„ë°•ë¹„ì¤‘']*100:.2f}%\n",
    "\"\"\"\n",
    "\n",
    "    if 'HH_ë¹„ìœ¨' in row.index:\n",
    "        report += f\"- **High-High ë¹„ìœ¨**: {row['HH_ë¹„ìœ¨']*100:.2f}%\\n\"\n",
    "\n",
    "report += \"\"\"\n",
    "---\n",
    "\n",
    "## ğŸ“ˆ 3. ê°€ì„¤ ê²€ì¦ ê²°ê³¼\n",
    "\n",
    "### âœ… ê°€ì„¤ 1: ì±„íƒ\n",
    "\n",
    "**ê°€ì„¤**: 5ê°œ ì§€í‘œêµ°ì— ë”°ë¥¸ ìƒê¶Œ ì„¸ë¶„í™”ê°€ ê°€ëŠ¥í•˜ë‹¤\n",
    "\n",
    "**ê·¼ê±°**:\n",
    "\"\"\"\n",
    "\n",
    "report += f\"\"\"\n",
    "1. âœ… {best_k}ê°œ í´ëŸ¬ìŠ¤í„°ë¡œ ì„¸ë¶„í™” ì„±ê³µ\n",
    "2. âœ… í´ëŸ¬ìŠ¤í„°ë³„ ì„ë°•ë¹„ì¤‘ ì°¨ì´ í™•ì¸\n",
    "3. âœ… ê³ ìœ„í—˜ í´ëŸ¬ìŠ¤í„° ì‹ë³„: {len(high_risk_clusters)}ê°œ\n",
    "4. âœ… Silhouette Score: {best_silhouette:.4f}\n",
    "\"\"\"\n",
    "\n",
    "if lisa_result:\n",
    "    report += f\"\"\"\n",
    "5. âœ… LISA ê³µê°„ë¶„ì„ ì™„ë£Œ\n",
    "   - High-High í•«ìŠ¤íŒŸ: {lisa_result['high_high_count']}ê°œ\n",
    "   - ìœ ì˜í•œ í•«ìŠ¤íŒŸ: {lisa_result['high_high_sig']}ê°œ\n",
    "\"\"\"\n",
    "\n",
    "report += \"\"\"\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ 4. ì •ì±…ì  ì‹œì‚¬ì \n",
    "\n",
    "### 4.1 ê³ ìœ„í—˜ í´ëŸ¬ìŠ¤í„° ì§‘ì¤‘ ì§€ì›\n",
    "\"\"\"\n",
    "\n",
    "if high_risk_clusters:\n",
    "    for cluster_id in high_risk_clusters:\n",
    "        row = cluster_risk.loc[cluster_id]\n",
    "        report += f\"\\n**í´ëŸ¬ìŠ¤í„° {cluster_id}**:\\n\"\n",
    "        report += f\"- ê°€ë§¹ì  ìˆ˜: {int(row['ê°€ë§¹ì ìˆ˜']):,}ê°œ\\n\"\n",
    "        report += f\"- ì„ë°•ë¹„ì¤‘: {row['ì„ë°•ë¹„ì¤‘']*100:.2f}%\\n\"\n",
    "        report += f\"- ì¦‰ê°ì ì¸ ê²½ì˜ ì»¨ì„¤íŒ… ë° ì¬ì • ì§€ì› í•„ìš”\\n\"\n",
    "else:\n",
    "    report += \"\\n- í˜„ì¬ ê³ ìœ„í—˜ í´ëŸ¬ìŠ¤í„° ì—†ìŒ (ì–‘í˜¸)\\n\"\n",
    "\n",
    "report += \"\"\"\n",
    "\n",
    "### 4.2 ì¡°ê¸° ê²½ë³´ ì‹œìŠ¤í…œ\n",
    "- í´ëŸ¬ìŠ¤í„°ë³„ ì„ë°•ë¹„ì¤‘ ëª¨ë‹ˆí„°ë§\n",
    "- ì„ë°•ë¹„ì¤‘ 5% ì´ìƒ â†’ ê²½ê³  ë°œë ¹\n",
    "\n",
    "### 4.3 ì§€ì—­ ê¸°ë°˜ ì •ì±… (LISA í™œìš©)\n",
    "\"\"\"\n",
    "\n",
    "if lisa_result:\n",
    "    report += \"\"\"\n",
    "- High-High í•«ìŠ¤íŒŸ ì§€ì—­ ìš°ì„  ì§€ì›\n",
    "- ê³µê°„ì  íŒŒê¸‰íš¨ê³¼ ì°¨ë‹¨\n",
    "\"\"\"\n",
    "else:\n",
    "    report += \"- LISA ë¶„ì„ ë¯¸ì‹¤ì‹œ (ì¢Œí‘œ ë°ì´í„° ë¶€ì¡±)\\n\"\n",
    "\n",
    "report += \"\"\"\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ ìƒì„± íŒŒì¼\n",
    "\n",
    "1. **í´ëŸ¬ìŠ¤í„°ë§_ê²°ê³¼_ì™„ì „íŒ.csv**: ê°€ë§¹ì ë³„ í´ëŸ¬ìŠ¤í„° + ìƒíƒœ + LISA\n",
    "2. **í´ëŸ¬ìŠ¤í„°_ì„ë°•ë¹„ì¤‘_ì™„ì „íŒ.csv**: í´ëŸ¬ìŠ¤í„°ë³„ ìœ„í—˜ë„ í†µê³„\n",
    "3. **í´ëŸ¬ìŠ¤í„°_ê°œìˆ˜_í‰ê°€_ì™„ì „íŒ.csv**: k=3~7 í‰ê°€\n",
    "4. **í´ëŸ¬ìŠ¤í„°_í”„ë¡œíŒŒì¼_ì™„ì „íŒ.csv**: í´ëŸ¬ìŠ¤í„°ë³„ ë³€ìˆ˜ í‰ê· \n",
    "5. **í´ëŸ¬ìŠ¤í„°ë§_ìš”ì•½_ì™„ì „íŒ.json**: JSON ìš”ì•½\n",
    "6. **ì´ íŒŒì¼**: ìƒì„¸ ë³´ê³ ì„œ\n",
    "\n",
    "---\n",
    "\n",
    "**ì‘ì„±ì**: Claude Code\n",
    "**ë¶„ì„ í”„ë ˆì„ì›Œí¬**: Python 3.12, scikit-learn, K-means, LISA\n",
    "**ì°¸ê³ **: í˜„ì„œë‹˜ ë ˆí¼ëŸ°ìŠ¤ ë°©ì‹ ì™„ì „ ì¬í˜„\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{output_dir}/ê°€ì„¤1_í´ëŸ¬ìŠ¤í„°ë§_ì™„ì „íŒ_ë³´ê³ ì„œ.md\", 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "print(\"âœ“ ìƒì„¸ ë³´ê³ ì„œ ì €ì¥\")\n",
    "\n",
    "# ============================================================================\n",
    "# ì™„ë£Œ\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ê°€ì„¤ 1 ì™„ì „íŒ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì™„ë£Œ!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"âœ… 5ë‹¨ê³„ ìƒíƒœ ë¼ë²¨ë§ ì™„ë£Œ\")\n",
    "print(f\"âœ… ìµœì  k: {best_k}\")\n",
    "print(f\"âœ… Silhouette Score: {best_silhouette:.4f}\")\n",
    "print(f\"âœ… ê³ ìœ„í—˜ í´ëŸ¬ìŠ¤í„°: {len(high_risk_clusters)}ê°œ\")\n",
    "if lisa_result:\n",
    "    print(f\"âœ… LISA High-High í•«ìŠ¤íŒŸ: {lisa_result['high_high_count']}ê°œ\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23920d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
